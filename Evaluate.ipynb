{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","authorship_tag":"ABX9TyMHxLgrXSbjIeUN3wUFcwPK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LYCMJG472fVb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1756786695790,"user_tz":-480,"elapsed":27288,"user":{"displayName":"Xiang Zhen","userId":"12352921533960858198"}},"outputId":"38c74f4d-519f-41dc-bdc7-d34abe67c91b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-5bEmBP2YXv"},"outputs":[],"source":["!pip install ftfy regex tqdm\n","#!pip install transformers\n","!pip install open_clip_torch\n","!pip install lpips\n","!pip install piq"]},{"cell_type":"markdown","source":["# Image Preprocessing"],"metadata":{"id":"vrDlegB_25fn"}},{"cell_type":"code","source":["import lpips\n","import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import torch.nn.functional as F\n","\n","\n","# Load image\n","def load_image(path):\n","    img = Image.open(path).convert('RGB')\n","    transform = transforms.Compose([\n","        transforms.Resize((512, 512)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","    return transform(img).unsqueeze(0)\n","\n","# Load image without normalize (for MS-SSIM)\n","def load_image_unNorm(path):\n","    img = Image.open(path).convert('RGB')\n","    t = transforms.Compose([transforms.Resize((512, 512)),\n","                            transforms.ToTensor()])\n","    return t(img).unsqueeze(0)\n"],"metadata":{"id":"POLpMnLr24sE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Metric"],"metadata":{"id":"rLS4CW6Wp_k3"}},{"cell_type":"code","source":["import lpips\n","import torch\n","import torchvision.transforms as transforms\n","import open_clip\n","from PIL import Image\n","from piq import multi_scale_ssim\n","from torchvision.utils import save_image\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","source_image_path = \"/Source_img.png\" # Source image path\n","target_image_path = \"/Target_img.png\" # Target image path\n","text_prompt = 'a lineart sketch'  # Prompt (for CLIP score)\n","\n","#LPIPS\n","Source_image = load_image(source_image_path)\n","Target_image = load_image(target_image_path)\n","lpips_model = lpips.LPIPS(net='alex').to(device)\n","Lpips = lpips_model(Source_image.to(device), Target_image.to(device), normalize=True)\n","\n","#MS-SSIM\n","Source_image_unNorm = load_image_unNorm(source_image_path)\n","Target_image_unNorm = load_image_unNorm(target_image_path)\n","MS_SSIM = multi_scale_ssim(Source_image_unNorm, Target_image_unNorm, data_range=1.0)\n","\n","#CLIP score\n","clip_model, _, clip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n","tokenizer = open_clip.get_tokenizer('ViT-B-32')\n","clip_model.eval().cuda()\n","\n","image = clip_preprocess(Image.open(source_image_path).convert(\"RGB\")).unsqueeze(0).cuda()\n","text = tokenizer([text_prompt]).cuda()\n","\n","with torch.no_grad():\n","    img_feat = clip_model.encode_image(image)\n","    txt_feat = clip_model.encode_text(text)\n","\n","    img_feat /= img_feat.norm(dim=-1, keepdim=True)\n","    txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n","\n","    CLIP_score = (img_feat @ txt_feat.T)\n","\n","\n","\n","print('MS_SSIM:',MS_SSIM.item()) # Much bigger much better\n","print('LPIPS:',Lpips.item())   # Much lower much better\n","print('CLIP_score:',CLIP_score.item()) # Much bigger much better\n"],"metadata":{"id":"3k_NAcYip_D5"},"execution_count":null,"outputs":[]}]}