{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyPw7OyaTqHR17DrFF8BITcc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SD62Dxov71Xk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755456759920,"user_tz":-60,"elapsed":24718,"user":{"displayName":"Xiang Zhen","userId":"12352921533960858198"}},"outputId":"12261321-8f61-4fb3-cec6-1e3b84390c6d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!time cp -r /content/drive/MyDrive/Dissertation/Dataset/Multiple_style3.zip /content/\n","!unzip -q /content/Multiple_style3.zip -d /content/"],"metadata":{"id":"XaRD3HlU74Xg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"I4X8-BLE8puN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install opencv-python\n","!pip install einops\n","!git clone https://github.com/lllyasviel/ControlNet-v1-1-nightly.git\n","!pip install --upgrade basicsr torchvision"],"metadata":{"id":"4xhpfxP1_oqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","sys.path.append(\"/content/ControlNet-v1-1-nightly\")"],"metadata":{"id":"xWfR8Ubgg6zB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"u3whuRoU8Vgi"}},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","from PIL import Image\n","import os\n","from transformers import CLIPTextModel, CLIPTokenizer\n","import torch\n","from einops import rearrange\n","from annotator.midas import MidasDetector\n","\n","class SketchDataset(Dataset):\n","    def __init__(self, root_dir, tokenizer, image_processor, size=512):\n","        self.Source_dir = os.path.join(root_dir, \"Source\")\n","        self.Sketch_dir = os.path.join(root_dir, \"Sketch\")\n","        self.Caption_dir = os.path.join(root_dir, \"Caption\")\n","\n","        self.filenames = sorted([\n","            fname.split('_')[0] for fname in os.listdir(self.Source_dir)\n","            if fname.endswith(('.jpg', '.png'))\n","        ])\n","\n","        self.tokenizer = tokenizer\n","        self.image_processor = image_processor\n","        self.size = size\n","        self.MidasDetector = MidasDetector()\n","        self.style2lambda = {\n","            \"pencil\": 0.9,\n","            \"architectural line drawing\": 0.5,\n","            \"anime lineart\":0.3\n","        }\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        fname = self.filenames[idx]\n","\n","        # Load image file\n","        Source_image = Image.open(os.path.join(self.Source_dir, fname + \"_left.png\")).convert(\"RGB\").resize((self.size, self.size))\n","        Sketch_image = Image.open(os.path.join(self.Sketch_dir, fname + \"_right.png\")).convert(\"RGB\").resize((self.size, self.size))\n","\n","        # Derive Canny map\n","        Canny_image = np.array(Source_image)\n","        Canny_image = cv2.Canny(Canny_image, 100, 200)\n","        Canny_image = cv2.cvtColor(Canny_image, cv2.COLOR_GRAY2RGB)\n","        Canny_image = torch.from_numpy(Canny_image).float() / 255.0\n","        Canny_image = rearrange(Canny_image, \"h w c ->  c h w\")\n","\n","        # Derive Depth map\n","        Depth_image = np.array(Source_image)\n","        Depth_image = self.MidasDetector(Depth_image)\n","        if Depth_image.ndim == 2:\n","          Depth_image = np.expand_dims(Depth_image, axis=2)\n","          Depth_image = np.repeat(Depth_image, 3, axis=2)\n","\n","        Depth_image = torch.from_numpy(Depth_image).float() / 255.0\n","        Depth_image = rearrange(Depth_image, \"h w c ->  c h w\")\n","\n","\n","        # Open text\n","        with open(os.path.join(self.Caption_dir, fname + \".txt\"), 'r', encoding='utf-8') as f:\n","            prompt = f.read().strip()\n","\n","        # Derive style caption\n","        style = \"\"\n","        try:\n","          start = prompt.lower().index(\"a \") + 2\n","          end = prompt.lower().index(\" sketch\", start)\n","          style = prompt[start:end].strip()\n","        except ValueError:\n","          style = \"UnKnown\"\n","\n","        # Get the text token\n","        Caption = self.tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\")\n","\n","        return {\n","            \"Canny_image\": Canny_image,\n","            \"Depth_image\": Depth_image,\n","            \"Sketch_image\": self.image_processor(Sketch_image),\n","            \"Caption_ids\": Caption.input_ids.squeeze(0),\n","            \"attention_mask\": Caption.attention_mask.squeeze(0),\n","            \"Style_lambda\": self.style2lambda[style],\n","            \"Style_name\": style\n","        }"],"metadata":{"id":"N_eMNh3i75z7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Avg meter"],"metadata":{"id":"EdZxLLrv8Zp6"}},{"cell_type":"code","source":["class AverageMeter(object):\n","  def __init__(self):\n","    self.reset()\n","\n","  def reset(self):\n","    self.val = 0\n","    self.avg = 0\n","    self.sum = 0\n","    self.count = 0\n","\n","  def update(self, val, n=1):\n","    self.val = val\n","    self.sum += val * n\n","    self.count += n\n","    self.avg = self.sum / self.count"],"metadata":{"id":"mVHRytuN79bv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loss Function"],"metadata":{"id":"7ASi_pQk8dKc"}},{"cell_type":"code","source":["import torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Huber_loss\n","def huber_loss(input, target, delta=1.0, reduction='mean'):\n","    diff = input - target\n","    abs_diff = diff.abs()\n","    quad = torch.clamp(abs_diff, max=delta)\n","    lin = abs_diff - quad\n","    loss = 0.5 * quad**2 + delta * lin\n","    if reduction == 'mean':\n","        return loss.mean()\n","    elif reduction == 'sum':\n","        return loss.sum()\n","    return loss\n","\n","\n","class SketchLoss(nn.Module):\n","    def __init__(\n","        self,\n","        latent_weight=1.0,        # MSE loss weight\n","        latent_l1_weight=0.0,      # L1 loss weight\n","        huber_weight=0.0,         # Huber loss weight\n","        huber_delta=1.0,         # Huber loss threshold\n","    ):\n","        super().__init__()\n","        self.latent_weight   = latent_weight\n","        self.latent_l1_weight= latent_l1_weight\n","        self.huber_weight    = huber_weight\n","        self.huber_delta     = huber_delta\n","\n","\n","    def forward(\n","        self,\n","        noise_pred,\n","        noise,\n","    ):\n","        total = 0.0\n","\n","        # MSE Loss\n","        if self.latent_weight > 0:\n","            latent_mse = F.mse_loss(noise_pred, noise)\n","            total = total + self.latent_weight * latent_mse\n","        else:\n","            latent_mse = noise_pred.new_zeros(())\n","\n","        # MAE Loss\n","        if self.latent_l1_weight > 0:\n","            latent_l1 = F.l1_loss(noise_pred, noise)\n","            total = total + self.latent_l1_weight * latent_l1\n","        else:\n","            latent_l1 = noise_pred.new_zeros(())\n","\n","        # Huber Loss\n","        if self.huber_weight > 0:\n","            latent_huber = huber_loss(noise_pred, noise, delta=self.huber_delta)\n","            total = total + self.huber_weight * latent_huber\n","        else:\n","            latent_huber = noise_pred.new_zeros(())\n","\n","        return total\n"],"metadata":{"id":"MM8cF2vk7_O8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set seed and Layer select"],"metadata":{"id":"sDY-_m_p8jwR"}},{"cell_type":"code","source":["import random, numpy as np, torch\n","\n","def set_seed(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  torch.backends.cudnn.deterministic = True\n","  torch.backends.cudnn.benchmark = False\n","  return torch.Generator(device=\"cuda\").manual_seed(seed)\n","\n","# Select mid layer\n","def get_mid_block_linear_modules(unet_model):\n","    target_modules = []\n","    for name, module in unet_model.named_modules():\n","        if \"mid_block\" in name and isinstance(module, torch.nn.Linear):\n","            if any(kw in name for kw in [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]):\n","                target_modules.append(name)\n","    return target_modules\n","\n","# Select up-sampling layer\n","def get_up_blocks_linear_modules(unet_model):\n","    target_modules = []\n","    for name, module in unet_model.named_modules():\n","        if \"up_blocks\" in name and isinstance(module, torch.nn.Linear):\n","          if any(kw in name for kw in [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]):\n","            target_modules.append(name)\n","    return target_modules\n","\n","# Select down-sampling layer\n","def get_down_blocks_linear_modules(unet_model):\n","    target_modules = []\n","    for name, module in unet_model.named_modules():\n","        if \"down_blocks\" in name and isinstance(module, torch.nn.Linear):\n","          if any(kw in name for kw in [\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"]):\n","            target_modules.append(name)\n","    return target_modules"],"metadata":{"id":"0tZdVO_U8BTq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training pipline"],"metadata":{"id":"z2xJ-OKr8SwJ"}},{"cell_type":"code","source":["from pickle import decode_long\n","from io import SEEK_SET\n","import torch\n","from torch.utils.data import DataLoader\n","from diffusers import StableDiffusionControlNetPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler, ControlNetModel\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from peft import get_peft_model, LoraConfig, TaskType\n","from torchvision import transforms\n","from tqdm.notebook import tqdm\n","from torch.utils.tensorboard import SummaryWriter\n","from PIL import Image\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","\n","# Configuration\n","model_id = \"runwayml/stable-diffusion-v1-5\"                   #Pretrained unet model id\n","controlmodel_id_1 = \"lllyasviel/control_v11p_sd15_canny\"             #Pretrained controlnet model id (for Canny map)\n","controlmodel_id_2 = \"lllyasviel/control_v11f1p_sd15_depth\"            #Pretrained controlnet model id (for depth map)\n","dataset_path = \"/Multiple_style3\"             #Dataset path\n","output_dir = \"/LoRA \"                   #Location to save the LoRA Matrix\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","batch_size = 1    # Batch size\n","lr = 5e-5      # Learning rate\n","max_steps = 5000   # Training step\n","cond_scale_1 = 0.5  # Canny map control intensity\n","cond_scale_2 = 1.0  # Depth map control intensity\n","seed = 5711\n","\n","# Load Pretrained Model\n","tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n","text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n","vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n","unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n","controlnet1 = ControlNetModel.from_pretrained(controlmodel_id_1).to(device)\n","controlnet2 = ControlNetModel.from_pretrained(controlmodel_id_2).to(device)\n","\n","noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n","set_seed(seed)\n","\n","\n","# LoRA Configuration\n","target_location = get_mid_block_linear_modules(unet)+get_up_blocks_linear_modules(unet) #Inject layer select\n","\n","lora_config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    target_modules=target_location, # Modified target modules, for all layer using [\"to_q\",\"to_k\",\"to_v\",\"to_out.0\"]\n","    lora_dropout=0.1,\n","    bias=\"none\"\n",")\n","unet = get_peft_model(unet, lora_config)\n","\n","# Data pre-processing\n","image_processor = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5], [0.5])\n","])\n","\n","# Load data\n","dataset = SketchDataset(dataset_path, tokenizer, image_processor)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","# Optimizer\n","optimizer = torch.optim.AdamW(unet.parameters(), lr=lr)\n","\n","# Training loop\n","step = 0\n","unet.train()\n","controlnet1.eval()\n","controlnet2.eval()\n","text_encoder.eval()\n","vae.eval()\n","train_loss = AverageMeter()\n","writer = SummaryWriter(\"runs/sketch_lora\")\n","for epoch in range(50):    # training epoch\n","    for batch in tqdm(dataloader, desc=f\"Epoch{epoch+1}\",leave=False):\n","        if step >= max_steps:\n","            break\n","\n","        Cond_image1 = batch[\"Canny_image\"].to(device)\n","        Cond_image2 = batch[\"Depth_image\"].to(device)\n","        Target_image = batch[\"Sketch_image\"].to(device)\n","        Input_ids = batch[\"Caption_ids\"].to(device)\n","        style_lambda = batch[\"Style_lambda\"].to(device)\n","\n","        # Encode to latent space\n","        latents = vae.encode(Target_image).latent_dist.sample() * 0.18215\n","\n","        # Add noise\n","        noise = torch.randn_like(latents)\n","        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n","        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","        # Get text embedding\n","        text_emb = text_encoder(Input_ids)[0]\n","\n","        # Predict noise\n","        down_block_res_samples_1, mid_block_res_sample_1 = controlnet1(\n","            sample=noisy_latents,\n","            timestep=timesteps,\n","            encoder_hidden_states=text_emb,\n","            controlnet_cond=Cond_image1,\n","            return_dict=False\n","            )\n","        down_block_res_samples_2, mid_block_res_sample_2 = controlnet2(\n","            sample=noisy_latents,\n","            timestep=timesteps,\n","            encoder_hidden_states=text_emb,\n","            controlnet_cond=Cond_image2,\n","            return_dict=False\n","        )\n","\n","        down_block_res_samples = [\n","            x1 * cond_scale_1+x2 * cond_scale_2 for x1, x2 in zip(down_block_res_samples_1,down_block_res_samples_2)\n","            ]\n","\n","        mid_block_res_sample_1 = mid_block_res_sample_1 * cond_scale_1\n","        mid_block_res_sample_2 = mid_block_res_sample_2 * cond_scale_2\n","\n","        noise_pred = unet(\n","            sample=noisy_latents,\n","            timestep=timesteps,\n","            encoder_hidden_states=text_emb,\n","            down_block_additional_residuals=down_block_res_samples,\n","            mid_block_additional_residual=mid_block_res_sample_1+mid_block_res_sample_2,\n","            ).sample\n","\n","        # Loss Configuration\n","        loss_fn = SketchLoss(\n","            latent_weight=style_lambda,\n","            latent_l1_weight=1-style_lambda,\n","            huber_weight=0,\n","            )\n","        loss = loss_fn(\n","            noise_pred=noise_pred,\n","            noise=noise\n","            )\n","\n","        # Backpropagation\n","        loss.backward()\n","\n","        optimizer.step()\n","        optimizer.zero_grad()\n","        train_loss.update(loss.item())\n","\n","        if step % 75 == 0:\n","            print(f\"Step {step}, Loss: {loss.item():.4f}, Avg_Loss:{train_loss.avg}\")\n","            writer.add_scalar(\"Avg_Loss/train\", train_loss.avg, step)\n","            writer.add_scalar(\"Loss/train\", loss.item(), step)\n","            train_loss.reset()\n","\n","        step += 1\n","\n","# Save model\n","writer.close()\n","unet.save_pretrained(output_dir)\n","print(\" Model training completed，saved in:\", output_dir)"],"metadata":{"id":"yiCiYydT8FNY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference pipline"],"metadata":{"id":"nTWwCkaF8QFI"}},{"cell_type":"code","source":["from io import SEEK_SET\n","import torch\n","from diffusers import StableDiffusionControlNetPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler, ControlNetModel\n","from transformers import CLIPTextModel, CLIPTokenizer\n","from peft import PeftModel,PeftConfig\n","from PIL import Image\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","from einops import rearrange\n","from annotator.midas import MidasDetector\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_id = \"runwayml/stable-diffusion-v1-5\"                  # pretrained unet model id\n","controlmodel_id_1 = \"lllyasviel/control_v11p_sd15_canny\"            # pretrained controlnet1 id (for canny map)\n","controlmodel_id_2 = \"lllyasviel/control_v11f1p_sd15_depth\"           # pretrained controlnet2 id (for depth map)\n","lora_path = \"/LoRA\"            # LoRA path\n","source_image_path = \"/Soure_img.png\"    # Source Image path\n","result_path = \"/output.png\"      # Generation result saving path\n","prompt = \"a pencil sketch of a room\"  # Prompt\n","num_inference_steps = 200  # denoise step\n","cond_scale_1 = 0.5   # Canny map control intensity\n","cond_scale_2 = 1.0   # Depth map control intensity\n","seed = 1589\n","\n","# Load pretrained model\n","tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n","text_encoder = CLIPTextModel.from_pretrained(model_id, subfolder=\"text_encoder\").to(device)\n","vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\").to(device)\n","unet = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\").to(device)\n","controlnet1 = ControlNetModel.from_pretrained(controlmodel_id_1).to(device)\n","controlnet2 = ControlNetModel.from_pretrained(controlmodel_id_2).to(device)\n","noise_scheduler = DDPMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n","\n","# Inject trained LoRA Matrix\n","unet = PeftModel.from_pretrained(unet, lora_path).to(device)\n","\n","# Input Image and Prompt\n","token=tokenizer(prompt, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\")\n","prompt_emb=text_encoder(token.input_ids.to(device))[0].to(device)\n","\n","source_image= Image.open(source_image_path).convert(\"RGB\").resize((512,512))\n","source_image = np.array(source_image)\n","\n","\n","# Derive Canny map\n","canny_image = cv2.Canny(source_image, 100, 200)\n","canny_image = cv2.cvtColor(canny_image, cv2.COLOR_GRAY2RGB)\n","canny_image = torch.from_numpy(canny_image).float() / 255.0\n","canny_image = rearrange(canny_image, 'h w c -> 1 c h w').to(device)\n","\n","# Derive Depth map\n","mida=MidasDetector()\n","depth_image = mida(source_image)\n","if depth_image.ndim == 2:\n","          depth_image = np.expand_dims(depth_image, axis=2)\n","          depth_image = np.repeat(depth_image, 3, axis=2)\n","depth_image = torch.from_numpy(depth_image).float() / 255.0\n","depth_image = rearrange(depth_image, \"h w c -> 1 c h w\").to(device)\n","\n","\n","# Inference\n","text_encoder.eval()\n","vae.eval()\n","unet.eval()\n","controlnet1.eval()\n","controlnet2.eval()\n","latents = torch.randn((1,4,64,64),device=device)\n","# gen = set_seed(seed)\n","# latents=torch.randn((1,4,64,64),generator=gen,device=device)\n","noise_scheduler.set_timesteps(num_inference_steps)\n","\n","\n","with torch.no_grad():\n","  for t in noise_scheduler.timesteps:\n","    down_block_res_samples_1, mid_block_res_sample_1 = controlnet1(\n","        sample=latents,\n","        timestep=t,\n","        encoder_hidden_states=prompt_emb,\n","        controlnet_cond=canny_image,\n","        return_dict=False\n","    )\n","\n","    down_block_res_samples_2, mid_block_res_sample_2 = controlnet2(\n","        sample=latents,\n","        timestep=t,\n","        encoder_hidden_states=prompt_emb,\n","        controlnet_cond=depth_image,\n","        return_dict=False\n","    )\n","\n","    down_block_res_samples = [\n","        x1 * cond_scale_1+x2 * cond_scale_2 for x1,x2 in zip(down_block_res_samples_1,down_block_res_samples_2)\n","        ]\n","    mid_block_res_sample_1 = mid_block_res_sample_1 * cond_scale_1\n","    mid_block_res_sample_2 = mid_block_res_sample_2 * cond_scale_2\n","    mid_block_res_sample = mid_block_res_sample_1+mid_block_res_sample_2\n","\n","    noise_inference_pred = unet(\n","        sample=latents,\n","        timestep=t,\n","        encoder_hidden_states=prompt_emb,\n","        down_block_additional_residuals= down_block_res_samples,\n","        mid_block_additional_residual= mid_block_res_sample,\n","    ).sample\n","\n","    latents = noise_scheduler.step(noise_inference_pred, t, latents).prev_sample # Corrected variable name\n","\n","  # Post processing\n","  image = vae.decode(latents/0.18215).sample\n","  r,g,b = image[:,0:1],image[:,1:2],image[:,2:3]\n","  gray = 0.2989*r + 0.5870*g + 0.1140*b\n","  enhance = gray + 0.3\n","  image_en = enhance.repeat(1,3,1,1)\n","  save_image((image_en+1)/2,result_path) # save enhanced image"],"metadata":{"id":"tlLuRTfX8IgV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1755458254459,"user_tz":-60,"elapsed":27880,"user":{"displayName":"Xiang Zhen","userId":"12352921533960858198"}},"outputId":"ea811afa-c7bd-4c4a-8ac1-663d3e5d0e34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-347325873.py:57: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n","  Image.fromarray(depth_image, mode='L').save(\"/content/depth_preview.png\")\n"]}]},{"cell_type":"markdown","source":["# Loss Curve"],"metadata":{"id":"S-8mMkWH8I-z"}},{"cell_type":"code","source":["#Launch the TensorBoard visualization\n","%reload_ext tensorboard\n","#Reading logs\n","%tensorboard --logdir runs/"],"metadata":{"id":"wqVHxMk58OsT"},"execution_count":null,"outputs":[]}]}